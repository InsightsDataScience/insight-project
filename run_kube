#!/bin/bash

# bail out if anything fails
set -e

setup_eks () {
  echo ">>> setup_eks starting..."

  echo ">>> provisioning infra with terraform..."
  cd terraform
  terraform init -input=false
  terraform apply -input=false -auto-approve

  echo ">>> configure kubectl for eks..."
  TF_OUTPUT=$(terraform output -json)
  CLUSTER_NAME="$(echo ${TF_OUTPUT} | jq -r .kubernetes_cluster_name.value)"
  aws eks update-kubeconfig --name ${CLUSTER_NAME}

  echo ">>> checking kubectl..."
  kubectl version
  kubectl get nodes
  kubectl config current-context
  kubectl config view

  echo ">>> setup_eks complete"
}

helm_init () {
  echo ">>> helm_init starting..."
  # initialize helm / tiller
  helm init
  kubectl create serviceaccount --namespace kube-system tiller
  kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
  kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
  echo ">>> helm_init complete"
}
setup_monitoring () {
  echo ">>> setup_monitoring starting..."

  kubectl create namespace monitoring
  
  # prometheus-operator: chart=v6.11.0, app=v0.32.0
  helm install stable/prometheus-operator\
  --name prom-monitor\
  --namespace monitoring\
  --set grafana.sidecar.dashboards.enabled=true\
  --set grafana.sidecar.dashboards.label=grafana_dashboard

  kubectl --namespace monitoring get pods -l "release=prom-monitor"

  echo ">>> setup_monitoring complete"
}

load_dashboards () {
  kubectl create cm chaos-dashboard --from-file=kubernetes/prom-operator/chaos-dashboard.json -n monitoring
  kubectl label cm chaos-dashboard grafana_dashboard=chaos-dashboard -n monitoring
}

delete_dashboards () {
  kubectl delete cm chaos-dashboard -n monitoring
}

cleanup_monitoring () {
  echo ">>> cleanup_monitoring starting..."
  # delete prometheus operator
  helm del --purge prom-monitor
  kubectl delete namespace monitoring
  echo ">>> cleanup_monitoring complete"
}

cleanup_tiller () {
  echo ">>> cleanup_tiller starting..."
  # delete cluster tiller
  kubectl get all --all-namespaces | grep tiller
  kubectl delete deployment tiller-deploy -n kube-system
  kubectl delete service tiller-deploy -n kube-system
  kubectl get all --all-namespaces | grep tiller
  echo ">>> cleanup_tiller complete"
}

teardown () {
  echo ">>> teardown starting..."

  echo ">>> destroying infra with terraform..."
  cd terraform
  terraform destroy -auto-approve

  echo ">>> cleanup..."
  rm -rf .terraform terraform.tfstate*
  rm -rf ~/.kube/config

  echo ">>> teardown complete"
}

debug () {
  if [ $1 == "delete_monitoring"]; then
    # delete prom-operator
    helm del --purge prom-monitor
  elif [ $1 == "reinstall_monitoring" ]; then
    # reinstall prom-operator
    helm install stable/prometheus-operator\
    --name prom-monitor\
    --namespace monitoring\
    --set prometheusOperator.createCustomResource=false\
    --set grafana.sidecar.dashboards.enabled=true\
    --set grafana.sidecar.dashboards.label=grafana_dashboard
  fi
}

"$@"
