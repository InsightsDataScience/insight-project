#Creating some variables to make the docker run command more readable
#App jar environment used by the spark-submit image
SPARK_APPLICATION_JAR_LOCATION="/opt/spark-apps/crimes-app.jar"
#App main class environment used by the spark-submit image
SPARK_APPLICATION_MAIN_CLASS="org.mvb.applications.CrimesApp"
#Extra submit args used by the spark-submit image
SPARK_SUBMIT_ARGS="--conf spark.executor.extraJavaOptions='-Dconfig-path=/opt/spark-apps/dev/config.conf'"

#We have to use the same network as the spark cluster(internally the image resolves spark master as spark://spark-master:7077)
docker run --network docker-spark-cluster_spark-network \
-v /mnt/spark-apps:/opt/spark-apps \
--env SPARK_APPLICATION_JAR_LOCATION=$SPARK_APPLICATION_JAR_LOCATION \
--env SPARK_APPLICATION_MAIN_CLASS=$SPARK_APPLICATION_MAIN_CLASS \
spark-submit:2.3.1

/spark/bin/spark-submit \
  --driver-memory 4g \
  --executor-memory 4g \
  --master spark://spark-master:7077 \
  /python/main.py \
  1000

/spark/bin/spark-shell --packages org.postgresql:postgresql:42.1.1
/spark/bin/spark-submit --driver-class-path /root/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar --master spark://spark-master:7077 /python/main.py 1000

# Table that maps song-hash to song-name
CREATE TABLE hash_name (
  hash character varying(80),
  song_name character varying(150),
  CONSTRAINT persons_pkey PRIMARY KEY (hash)
);

# Load data into `hash_name`
\copy hash_name FROM 'hash_names.csv' with (format csv,header true, delimiter ',');

# Table that maps midi-instrument-code to instrument-name
CREATE TABLE midi_instrument(
	code VARCHAR (5) UNIQUE NOT NULL,
	name VARCHAR (80) NOT NULL,
	family VARCHAR (55) NOT NULL
);

# Load data into midi_instrument
\copy midi_instrument FROM 'midi_instruments.csv' with (format csv,header true, delimiter ',');

CREATE TABLE filename_instrument(
  filename VARCHAR (80) NOT NULL,
  instrument VARCHAR (80) NOT NULL
);