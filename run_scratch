#!/bin/bash

# bail out if anything fails
set -e

# helm install --name <release> stable/<chart> --set prometheusOperator.createCustomResource=false -f <yaml>

# helm upgrade -f <yaml> <release> stable/<chart>

# kubectl port-forward -n monitoring deploy/prometheus-server 9090:9090

# grafana on localhost:3000
# kubectl port-forward <grafana_pod> -n monitoring 3000:3000

# get grafana admin pw
# default grafana creds: admin:prom-operator
# kubectl get secret --namespace grafana grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

# In grafana dashboard
# - click '+' & select 'import' 
# - 3131 for 'Kubernetes All Nodes' Dashboard
# - 3146 for 'Kubernetes Pods' Dashboard
# - select 'Prometheus' under data sources
# - click 'import'

old_setup_monitoring () {
  echo ">>> setup_monitoring starting..."

  kubectl create namespace monitoring

  # install prometheus
  helm install stable/prometheus \
      --name prometheus \
      --namespace monitoring \
      --set alertmanager.persistentVolume.storageClass="gp2" \
      --set server.persistentVolume.storageClass="gp2"

  # install grafana
  helm install stable/grafana \
      --name grafana \
      --namespace monitoring \
      --set persistence.storageClassName="gp2" \
      --set adminPassword="notadmin" \
      --set datasources."datasources\.yaml".apiVersion=1 \
      --set datasources."datasources\.yaml".datasources[0].name=Prometheus \
      --set datasources."datasources\.yaml".datasources[0].type=prometheus \
      --set datasources."datasources\.yaml".datasources[0].url=http://prometheus-server.prometheus.svc.cluster.local \
      --set datasources."datasources\.yaml".datasources[0].access=proxy \
      --set datasources."datasources\.yaml".datasources[0].isDefault=true \
      --set service.type=LoadBalancer

  # check prometheus namespace
  kubectl get all -n monitoring

  # debug
  #kubectl port-forward -n prometheus deploy/prometheus-server 8080:9090

  # get grafana elb url
  export ELB=$(kubectl get svc -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
  echo "grafana_elb_url: http://$ELB"

  echo ">>> setup_monitoring complete"
}

containerize () {
  DOCKER_USER=ajgrande924
  # docker login
  cd app/spark/master
  docker build -t ${DOCKER_USER}/spark-master .
  # docker push ${DOCKER_USER}/spark-master
}

test_jump_pod () {
  kubectl create namespace spark-pi
  kubectl apply -f kubernetes/spark-exp/spark-role.yaml
  kubectl auth can-i create pod --as=system:serviceaccount:spark-pi:spark-pi -n spark-pi

  # jump pod
  kubectl run --generator=run-pod/v1 jump-pod --rm -i --tty --serviceaccount=spark-pi --namespace=spark-pi --image vitamingaugau/spark:spark-2.4.4 sh

  # inside jump pod
  export SA=spark-pi
  export NAMESPACE=spark-pi
  export TOKEN=/var/run/secrets/kubernetes.io/serviceaccount/token
  export CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt

  # scala jar example
  /opt/spark/bin/spark-submit \
    --master=k8s://https://2003D3175F75CB25EE1389647B0467C6.gr7.us-west-2.eks.amazonaws.com:443 \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.kubernetes.driver.pod.name=spark-pi-driver  \
    --conf spark.kubernetes.container.image=vitamingaugau/spark:spark-2.4.4 \
    --conf spark.kubernetes.namespace=$NAMESPACE \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=$SA \
    --conf spark.kubernetes.authenticate.submission.caCertFile=$CACERT \
    --conf spark.kubernetes.authenticate.submission.oauthTokenFile=$TOKEN \
    --conf spark.executor.instances=2 \
    local:///opt/spark/examples/target/scala-2.11/jars/spark-examples_2.11-2.4.4.jar 20000
}

build_spark () {
  docker container run \
    --privileged -it \
    --name spark-build \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ${PWD}/app/spark:/tmp \
    -e USER=${SECRET_USER} \
    -e PASSWORD=${SECRET_PW} \
    -w /opt \
    docker:dind \
    sh /tmp/build.sh
}

"$@"
